{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install unsloth\n",
        "# Also get the latest nightly Unsloth!\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git"
      ],
      "metadata": {
        "id": "cC6DXEgwYl2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hsHRZB8SW5_Z"
      },
      "outputs": [],
      "source": [
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install peft"
      ],
      "metadata": {
        "id": "CEhgiIXnzf4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from unsloth import FastLanguageModel\n",
        "from transformers import TextStreamer\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "# model, tokenizer = FastLanguageModel.from_pretrained(\"lora_model\")\n",
        "from peft import PeftModel\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\")\n",
        "model = PeftModel.from_pretrained(base_model, \"DarkAngel/gitallama\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\")"
      ],
      "metadata": {
        "id": "BHvacq2eW97g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\")\n",
        "def generate_response(shloka, transliteration):\n",
        "    \"\"\"\n",
        "    Generates the response using the fine-tuned LLaMA model.\n",
        "    \"\"\"\n",
        "    input_message = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"Shloka: {shloka} Transliteration: {transliteration}\"\n",
        "        }\n",
        "    ]\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        input_message,\n",
        "        tokenize=True,\n",
        "        add_generation_prompt=True,  # Enable for generation\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(\"cuda\")  # Assuming the model is running on GPU\n",
        "\n",
        "    # Generate response\n",
        "    text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
        "    generated_tokens = model.generate(\n",
        "        input_ids=inputs,\n",
        "        streamer=text_streamer,\n",
        "        max_new_tokens=512,\n",
        "        use_cache=True,\n",
        "        temperature=1.5,\n",
        "        min_p=0.1\n",
        "    )\n",
        "\n",
        "    raw_response = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
        "\n",
        "    # Format the response\n",
        "    # Assuming raw_response contains English Meaning, Hindi Meaning, and Word Meaning in sequence\n",
        "    try:\n",
        "        sections = raw_response.split(\"Hindi Meaning:\")\n",
        "        english_meaning = sections[0].strip()\n",
        "        hindi_and_word = sections[1].split(\"Word Meaning:\")\n",
        "        hindi_meaning = hindi_and_word[0].strip()\n",
        "        word_meaning = hindi_and_word[1].strip()\n",
        "\n",
        "        # Format response for better readability\n",
        "        formatted_response = (\n",
        "            f\"English Meaning:\\n{english_meaning}\\n\\n\"\n",
        "            f\"Hindi Meaning:\\n{hindi_meaning}\\n\\n\"\n",
        "            f\"Word Meaning:\\n{word_meaning}\"\n",
        "        )\n",
        "    except IndexError:\n",
        "        # In case the response format is not as expected\n",
        "        formatted_response = raw_response\n",
        "\n",
        "    return formatted_response\n",
        "\n",
        "# Gradio interface\n",
        "interface = gr.Interface(\n",
        "    fn=generate_response,\n",
        "    inputs=[\n",
        "        gr.Textbox(label=\"Enter Shloka\", placeholder=\"Type or paste a Shloka here\"),\n",
        "        gr.Textbox(label=\"Enter Transliteration\", placeholder=\"Type or paste the transliteration here\")\n",
        "    ],\n",
        "    outputs=gr.Textbox(label=\"Generated Response\"),\n",
        "    title=\"Bhagavad Gita LLaMA Model\",\n",
        "    description=\"Input a Shloka with its transliteration, and this model will provide meanings in English and Hindi along with word meanings.\"\n",
        ")\n",
        "\n",
        "# Launch the interface\n",
        "if __name__ == \"__main__\":\n",
        "    interface.launch()\n"
      ],
      "metadata": {
        "id": "maFS_2CY6t7c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}